{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.contrib import tzip\n",
    "\n",
    "from helper_functions import get_tess_df, get_features, get_crema_df, get_my_audio\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "my_audio_df = get_my_audio()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "tess_df = get_tess_df()\n",
    "crema_df = get_crema_df()\n",
    "\n",
    "data_path = pd.concat([crema_df, tess_df], axis=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from helper_functions import create_images\n",
    "\n",
    "emotion='angry'\n",
    "path = \"TESS/OAF_angry/OAF_back_angry.wav\"\n",
    "create_images([path, \"TESS/OAF_Fear/OAF_back_fear.wav\"], [emotion, 'fear'], figsize=(6, 10))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.title('Count of Emotions', size=16)\n",
    "sns.countplot(x='Emotions', data=data_path)\n",
    "plt.ylabel('Count', size=12)\n",
    "plt.xlabel('Emotions', size=12)\n",
    "sns.despine(top=True, right=True, left=False, bottom=False)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X, Y = [], []\n",
    "\n",
    "for path, emotion in tzip(data_path.Path, data_path.Emotions):\n",
    "    feature = get_features(path)\n",
    "    for ele in feature:\n",
    "        X.append(ele)\n",
    "        # appending emotion 3 times as we have made 3 augmentation techniques on each audio file.\n",
    "        Y.append(emotion)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(X[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#pickle.dump((X, Y), open('extracted_features.pkl', 'wb'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "X, Y = pickle.load(open('extracted_features.pkl', 'rb'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(X), len(Y), data_path.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0           1           2          3          4          5  \\\n",
      "0  0.097949 -262.274261  111.292137  -0.619845  48.630657 -10.620001   \n",
      "1  0.292554 -101.032630   28.685769   8.788258  12.950129  -0.353596   \n",
      "2  0.101923 -316.482178  100.732925  -1.121063  44.560493 -13.020816   \n",
      "3  0.093924 -342.439514  130.888840   8.968784  52.469501 -19.662062   \n",
      "4  0.250042 -179.444821   34.033375  18.841786  11.593927   1.859677   \n",
      "\n",
      "          6          7         8          9  ...           173           174  \\\n",
      "0 -0.285672 -17.527323 -1.120136 -13.054270  ...  3.973798e-08  4.090360e-08   \n",
      "1 -9.653648  -9.215764 -4.868280  -7.088598  ...  5.239636e-02  5.605551e-02   \n",
      "2 -2.672002 -17.052134 -0.555105 -14.461847  ...  3.779767e-08  3.913161e-08   \n",
      "3  9.624663 -18.212955  2.013108 -10.330219  ...  1.706682e-08  1.628212e-08   \n",
      "4 -3.818330  -6.092026 -3.902650  -5.204026  ...  1.326073e-02  1.300624e-02   \n",
      "\n",
      "            175           176           177           178           179  \\\n",
      "0  4.180675e-08  4.244493e-08  4.289032e-08  4.322475e-08  4.343712e-08   \n",
      "1  5.762654e-02  5.527193e-02  5.684073e-02  5.711545e-02  5.499243e-02   \n",
      "2  4.003576e-08  4.083760e-08  4.058086e-08  3.546466e-08  2.366310e-08   \n",
      "3  1.564078e-08  1.509551e-08  1.464057e-08  1.428129e-08  1.399791e-08   \n",
      "4  1.227063e-02  1.340564e-02  1.382437e-02  1.354251e-02  1.322558e-02   \n",
      "\n",
      "            180           181  labels  \n",
      "0  4.359465e-08  4.366131e-08   angry  \n",
      "1  5.843149e-02  5.307063e-02   angry  \n",
      "2  1.291836e-08  8.599036e-09   angry  \n",
      "3  1.380088e-08  1.367411e-08   angry  \n",
      "4  1.297259e-02  1.334674e-02   angry  \n",
      "\n",
      "[5 rows x 183 columns]\n"
     ]
    }
   ],
   "source": [
    "from helper_functions import encode_emotion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "features = pd.DataFrame(X)\n",
    "features['labels'] = Y\n",
    "#features.to_csv('features.csv', index=False)\n",
    "print(features.head())\n",
    "\n",
    "X_t = features.iloc[:, :-1].values\n",
    "Y_tt = features['labels'].values\n",
    "\n",
    "Y_t = []\n",
    "for iii in range(len(Y_tt)):\n",
    "    Y_t.append(encode_emotion(Y_tt[iii]))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_t, Y_t, random_state=0, shuffle=True)\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "rf_scores = []\n",
    "best_rf_clf = None\n",
    "best_accuracy = 0\n",
    "best_iii = 0\n",
    "for iii in tqdm(range(10, 51)):\n",
    "    rf_clf = RandomForestClassifier(n_estimators=iii)\n",
    "    cross_score = cross_val_score(rf_clf, X_t, Y_t)\n",
    "    cross_mean = cross_score.mean()\n",
    "\n",
    "    rf_scores.append(cross_mean)\n",
    "\n",
    "    print(f\"{iii} cross_mean: {cross_mean}\")\n",
    "\n",
    "    if cross_mean > best_accuracy:\n",
    "        best_accuracy = cross_mean\n",
    "        best_rf_clf = rf_clf\n",
    "        best_iii = iii"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Best n_estimators for RF {best_iii}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(range(10, 51), rf_scores)\n",
    "plt.title(\"Random Forest\")\n",
    "plt.xlabel(\"Number of estimators\")\n",
    "plt.ylabel(\"Mean of cross validation\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(best_accuracy)\n",
    "print(best_iii)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_rf_clf.fit(x_train, y_train)\n",
    "rf_prediction = best_rf_clf.predict(x_test)\n",
    "print(rf_prediction[0])\n",
    "print(accuracy_score(rf_prediction, y_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pickle.dump(best_rf_clf, open('best_rf_clf.pkl', 'wb'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "clf = pickle.load(open('best_rf_clf.pkl', 'rb'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/davidtrafela/PycharmProjects/SU-emotion-recognition/my_audio/David_neutral.m4a\n",
      "neutral\n",
      "[[0.02083333 0.29166667 0.25       0.04166667 0.0625     0.3125\n",
      "  0.02083333]]\n",
      "/Users/davidtrafela/PycharmProjects/SU-emotion-recognition/my_audio/Hi_David_sad.m4a\n",
      "sad\n",
      "[[0.16666667 0.08333333 0.20833333 0.0625     0.125      0.27083333\n",
      "  0.08333333]]\n",
      "/Users/davidtrafela/PycharmProjects/SU-emotion-recognition/my_audio/David_angry.m4a\n",
      "angry\n",
      "[[0.29166667 0.04166667 0.08333333 0.         0.1875     0.35416667\n",
      "  0.04166667]]\n",
      "/Users/davidtrafela/PycharmProjects/SU-emotion-recognition/my_audio/David_sad.m4a\n",
      "sad\n",
      "[[0.125      0.27083333 0.125      0.         0.10416667 0.22916667\n",
      "  0.14583333]]\n",
      "/Users/davidtrafela/PycharmProjects/SU-emotion-recognition/my_audio/Hi_David_happy.m4a\n",
      "happy\n",
      "[[0.16666667 0.04166667 0.25       0.04166667 0.14583333 0.29166667\n",
      "  0.0625    ]]\n",
      "/Users/davidtrafela/PycharmProjects/SU-emotion-recognition/my_audio/Hi_David_neutral.m4a\n",
      "neutral\n",
      "[[0.14583333 0.0625     0.33333333 0.02083333 0.08333333 0.25\n",
      "  0.10416667]]\n"
     ]
    }
   ],
   "source": [
    "from helper_functions import predict_for_model\n",
    "\n",
    "predict_for_model(clf, scaler, my_audio_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from helper_functions import prepare_for_prediction, decode_emotion\n",
    "\n",
    "for path, emotion in zip(my_audio_df.Path, my_audio_df.Emotions):\n",
    "    print(path)\n",
    "    print(emotion)\n",
    "    features_my = [prepare_for_prediction(path)]\n",
    "    #print(features_my)\n",
    "    features_my = pd.DataFrame(features_my)\n",
    "    features_my['labels'] = emotion\n",
    "    #print(features_my.head)\n",
    "\n",
    "    my_case_x = scaler.transform(features_my.iloc[:, :-1].values)\n",
    "    prediction = clf.predict_proba(my_case_x)\n",
    "    print(prediction)\n",
    "    #print(decode_emotion(prediction[0]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "extra_trees_scores = []\n",
    "best_extra_trees_clf = None\n",
    "best_accuracy = 0\n",
    "best_iii = 49\n",
    "for iii in tqdm(range(45, 70)):\n",
    "    extra_trees_clf = ExtraTreesClassifier(n_estimators=iii)\n",
    "    cross_score = cross_val_score(extra_trees_clf, X_t, Y_t)\n",
    "    cross_mean = cross_score.mean()\n",
    "\n",
    "    extra_trees_scores.append(cross_mean)\n",
    "\n",
    "    print(f\"{iii} cross_mean: {cross_mean}\")\n",
    "\n",
    "    if cross_mean > best_accuracy:\n",
    "        best_accuracy = cross_mean\n",
    "        best_extra_trees_clf = extra_trees_clf\n",
    "        best_iii = iii"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Best n_estimators for ExtraTreesClassifier {best_iii}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(range(45, 70), extra_trees_scores)\n",
    "plt.title(\"ExtraTreesClassifier\")\n",
    "plt.xlabel(\"Number of estimators\")\n",
    "plt.ylabel(\"Mean of cross validation\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_extra_trees_clf.fit(x_train, y_train)\n",
    "extra_trees_prediction = best_extra_trees_clf.predict(x_test)\n",
    "print(extra_trees_prediction[0])\n",
    "print(accuracy_score(extra_trees_prediction, y_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pickle.dump(best_gradient_clf, open('best_gradient_clf.pkl', 'wb'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf = pickle.load(open('best_gradient_clf.pkl', 'rb'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/davidtrafela/PycharmProjects/SU-emotion-recognition/my_audio/David_neutral.m4a\n",
      "neutral\n",
      "[[0.02083333 0.29166667 0.25       0.04166667 0.0625     0.3125\n",
      "  0.02083333]]\n",
      "/Users/davidtrafela/PycharmProjects/SU-emotion-recognition/my_audio/Hi_David_sad.m4a\n",
      "sad\n",
      "[[0.16666667 0.08333333 0.20833333 0.0625     0.125      0.27083333\n",
      "  0.08333333]]\n",
      "/Users/davidtrafela/PycharmProjects/SU-emotion-recognition/my_audio/David_angry.m4a\n",
      "angry\n",
      "[[0.29166667 0.04166667 0.08333333 0.         0.1875     0.35416667\n",
      "  0.04166667]]\n",
      "/Users/davidtrafela/PycharmProjects/SU-emotion-recognition/my_audio/David_sad.m4a\n",
      "sad\n",
      "[[0.125      0.27083333 0.125      0.         0.10416667 0.22916667\n",
      "  0.14583333]]\n",
      "/Users/davidtrafela/PycharmProjects/SU-emotion-recognition/my_audio/Hi_David_happy.m4a\n",
      "happy\n",
      "[[0.16666667 0.04166667 0.25       0.04166667 0.14583333 0.29166667\n",
      "  0.0625    ]]\n",
      "/Users/davidtrafela/PycharmProjects/SU-emotion-recognition/my_audio/Hi_David_neutral.m4a\n",
      "neutral\n",
      "[[0.14583333 0.0625     0.33333333 0.02083333 0.08333333 0.25\n",
      "  0.10416667]]\n"
     ]
    }
   ],
   "source": [
    "predict_for_model(clf, scaler, my_audio_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_scores = []\n",
    "best_ada_clf = None\n",
    "best_accuracy = 0\n",
    "best_iii = 49\n",
    "for iii in tqdm(range(10, 51)):\n",
    "    ada_clf = AdaBoostClassifier(n_estimators=iii)\n",
    "    cross_score = cross_val_score(ada_clf, X_t, Y_t)\n",
    "    cross_mean = cross_score.mean()\n",
    "\n",
    "    ada_scores.append(cross_mean)\n",
    "\n",
    "    print(f\"{iii} cross_mean: {cross_mean}\")\n",
    "\n",
    "    if cross_mean > best_accuracy:\n",
    "        best_accuracy = cross_mean\n",
    "        best_ada_clf = ada_clf\n",
    "        best_iii = iii"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Best n_estimators for AdaBoost {best_iii}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(range(10, 51), ada_scores)\n",
    "plt.title(\"AdaBoost\")\n",
    "plt.xlabel(\"Number of estimators\")\n",
    "plt.ylabel(\"Mean of cross validation\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_ada_clf.fit(x_train, y_train)\n",
    "ada_prediction = best_ada_clf.predict(x_test)\n",
    "print(ada_prediction[0])\n",
    "print(accuracy_score(ada_prediction, y_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pickle.dump(best_ada_clf, open('best_ada_clf.pkl', 'wb'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/davidtrafela/PycharmProjects/SU-emotion-recognition/my_audio/David_neutral.m4a\n",
      "neutral\n",
      "[[0.15733193 0.14704257 0.15875289 0.14474621 0.14591753 0.16426584\n",
      "  0.08194303]]\n",
      "/Users/davidtrafela/PycharmProjects/SU-emotion-recognition/my_audio/Hi_David_sad.m4a\n",
      "sad\n",
      "[[0.1501214  0.14619938 0.16139264 0.14987774 0.13753017 0.15384868\n",
      "  0.10102999]]\n",
      "/Users/davidtrafela/PycharmProjects/SU-emotion-recognition/my_audio/David_angry.m4a\n",
      "angry\n",
      "[[0.164369   0.14019527 0.15865333 0.13656711 0.14941879 0.1617321\n",
      "  0.08906439]]\n",
      "/Users/davidtrafela/PycharmProjects/SU-emotion-recognition/my_audio/David_sad.m4a\n",
      "sad\n",
      "[[0.15224476 0.14376313 0.15829917 0.1444852  0.1301907  0.15885076\n",
      "  0.11216629]]\n",
      "/Users/davidtrafela/PycharmProjects/SU-emotion-recognition/my_audio/Hi_David_happy.m4a\n",
      "happy\n",
      "[[0.16304492 0.14007065 0.16123246 0.13610921 0.13941323 0.1599567\n",
      "  0.10017283]]\n",
      "/Users/davidtrafela/PycharmProjects/SU-emotion-recognition/my_audio/Hi_David_neutral.m4a\n",
      "neutral\n",
      "[[0.1501214  0.14619938 0.16139264 0.14987774 0.13753017 0.15384868\n",
      "  0.10102999]]\n"
     ]
    }
   ],
   "source": [
    "clf = pickle.load(open('best_ada_clf.pkl', 'rb'))\n",
    "predict_for_model(clf, scaler, my_audio_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "((23044, 182, 1), (23044, 7), (7682, 182, 1), (7682, 7))"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
    "\n",
    "x_train_t = np.expand_dims(np.asarray(x_train), axis=2)\n",
    "x_test_t = np.expand_dims(np.asarray(x_test), axis=2)\n",
    "\n",
    "y_train_t = np.asarray(y_train)\n",
    "y_test_t = np.asarray(y_test)\n",
    "y_train_t = to_categorical(y_train_t, 7)\n",
    "y_test_t = to_categorical(y_test_t, 7)\n",
    "\n",
    "x_train_t.shape, y_train_t.shape, x_test_t.shape, y_test_t.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, Dropout, Flatten, Dense\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras import Sequential\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(x_train_t.shape[1], 1)))\n",
    "model.add(MaxPooling1D(pool_size=5, strides=2, padding='same'))\n",
    "\n",
    "model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5, strides=2, padding='same'))\n",
    "\n",
    "model.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5, strides=2, padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5, strides=2, padding='same'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=32, activation='swish'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(units=7, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pickle.dump(model, open('nn_model.pkl', 'wb'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "rlrp = ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=1, patience=2, min_lr=0.000001)\n",
    "history = model.fit(x_train_t, y_train_t, batch_size=64, epochs=50, validation_split=0.15, callbacks=[rlrp])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluation = model.evaluate(x_test_t, y_test_t)\n",
    "print(evaluation)\n",
    "print(\"Accuracy of our model on test data : \", evaluation[1] * 100, \"%\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs = [i for i in range(50)]\n",
    "fig, ax = plt.subplots(2, 1)\n",
    "train_acc = history.history['accuracy']\n",
    "train_loss = history.history['loss']\n",
    "test_acc = history.history['val_accuracy']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "fig.set_size_inches(10, 10)\n",
    "ax[0].plot(epochs, train_loss, label='Training Loss')\n",
    "ax[0].plot(epochs, test_loss, label='Validattion Loss')\n",
    "ax[0].set_title('Training & Validation Loss')\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel(\"Epochs\")\n",
    "\n",
    "ax[1].plot(epochs, train_acc, label='Training Accuracy')\n",
    "ax[1].plot(epochs, test_acc, label='Validation Accuracy')\n",
    "ax[1].set_title('Training & Validation Accuracy')\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel(\"Epochs\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras model archive loading:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2022-12-22 12:44:04         4819\n",
      "metadata.json                                  2022-12-22 12:44:04           64\n",
      "variables.h5                                   2022-12-22 12:44:04      6760736\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
      "...layers\n",
      "......conv1d\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv1d_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv1d_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv1d_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dropout\n",
      ".........vars\n",
      "......dropout_1\n",
      ".........vars\n",
      "......flatten\n",
      ".........vars\n",
      "......max_pooling1d\n",
      ".........vars\n",
      "......max_pooling1d_1\n",
      ".........vars\n",
      "......max_pooling1d_2\n",
      ".........vars\n",
      "......max_pooling1d_3\n",
      ".........vars\n",
      "...metrics\n",
      "......mean\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......mean_metric_wrapper\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "...optimizer\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........10\n",
      ".........11\n",
      ".........12\n",
      ".........13\n",
      ".........14\n",
      ".........15\n",
      ".........16\n",
      ".........17\n",
      ".........18\n",
      ".........19\n",
      ".........2\n",
      ".........20\n",
      ".........21\n",
      ".........22\n",
      ".........23\n",
      ".........24\n",
      ".........3\n",
      ".........4\n",
      ".........5\n",
      ".........6\n",
      ".........7\n",
      ".........8\n",
      ".........9\n",
      "...vars\n",
      "/Users/davidtrafela/PycharmProjects/SU-emotion-recognition/my_audio/David_neutral.m4a\n",
      "neutral\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "[[1. 0. 1. 0. 0. 1. 0.]]\n",
      "/Users/davidtrafela/PycharmProjects/SU-emotion-recognition/my_audio/Hi_David_sad.m4a\n",
      "sad\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "[[0. 0. 1. 0. 0. 0. 0.]]\n",
      "/Users/davidtrafela/PycharmProjects/SU-emotion-recognition/my_audio/David_angry.m4a\n",
      "angry\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "[[1. 0. 1. 0. 0. 1. 0.]]\n",
      "/Users/davidtrafela/PycharmProjects/SU-emotion-recognition/my_audio/David_sad.m4a\n",
      "sad\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "[[1. 0. 1. 0. 0. 1. 0.]]\n",
      "/Users/davidtrafela/PycharmProjects/SU-emotion-recognition/my_audio/Hi_David_happy.m4a\n",
      "happy\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "[[1. 0. 1. 0. 0. 0. 0.]]\n",
      "/Users/davidtrafela/PycharmProjects/SU-emotion-recognition/my_audio/Hi_David_neutral.m4a\n",
      "neutral\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "[[1. 0. 1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "clf = pickle.load(open('nn_model.pkl', 'rb'))\n",
    "predict_for_model(clf, scaler, my_audio_df, nn=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(x_train_t.shape[1], 1)))\n",
    "model2.add(MaxPooling1D(pool_size=5, strides=2, padding='same'))\n",
    "\n",
    "model2.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "model2.add(MaxPooling1D(pool_size=5, strides=2, padding='same'))\n",
    "model2.add(Dropout(0.2))\n",
    "\n",
    "model2.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "model2.add(MaxPooling1D(pool_size=5, strides=2, padding='same'))\n",
    "\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(units=32, activation='swish'))\n",
    "model2.add(Dropout(0.3))\n",
    "\n",
    "model2.add(Dense(units=7, activation='sigmoid'))\n",
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "307/307 [==============================] - 38s 114ms/step - loss: 1.4084 - accuracy: 0.4326 - val_loss: 1.1954 - val_accuracy: 0.5120 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "307/307 [==============================] - 44s 143ms/step - loss: 1.1926 - accuracy: 0.5259 - val_loss: 1.0934 - val_accuracy: 0.5745 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "307/307 [==============================] - 42s 138ms/step - loss: 1.1284 - accuracy: 0.5497 - val_loss: 1.0543 - val_accuracy: 0.5771 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "307/307 [==============================] - 35s 113ms/step - loss: 1.0956 - accuracy: 0.5627 - val_loss: 1.0340 - val_accuracy: 0.5939 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "307/307 [==============================] - 40s 129ms/step - loss: 1.0650 - accuracy: 0.5791 - val_loss: 1.0152 - val_accuracy: 0.5968 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "307/307 [==============================] - 40s 130ms/step - loss: 1.0447 - accuracy: 0.5831 - val_loss: 0.9880 - val_accuracy: 0.6046 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "307/307 [==============================] - 41s 132ms/step - loss: 1.0232 - accuracy: 0.5926 - val_loss: 0.9972 - val_accuracy: 0.6046 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "307/307 [==============================] - 40s 131ms/step - loss: 1.0134 - accuracy: 0.5964 - val_loss: 0.9927 - val_accuracy: 0.6078 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "307/307 [==============================] - 33s 107ms/step - loss: 1.0012 - accuracy: 0.6008 - val_loss: 0.9687 - val_accuracy: 0.6234 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "307/307 [==============================] - 34s 111ms/step - loss: 0.9906 - accuracy: 0.6098 - val_loss: 0.9699 - val_accuracy: 0.6138 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "307/307 [==============================] - 36s 118ms/step - loss: 0.9706 - accuracy: 0.6171 - val_loss: 0.9617 - val_accuracy: 0.6213 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "307/307 [==============================] - 32s 104ms/step - loss: 0.9615 - accuracy: 0.6169 - val_loss: 0.9747 - val_accuracy: 0.6202 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "307/307 [==============================] - 31s 101ms/step - loss: 0.9521 - accuracy: 0.6259 - val_loss: 0.9522 - val_accuracy: 0.6297 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "307/307 [==============================] - 30s 98ms/step - loss: 0.9365 - accuracy: 0.6290 - val_loss: 0.9702 - val_accuracy: 0.6335 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "307/307 [==============================] - 30s 98ms/step - loss: 0.9405 - accuracy: 0.6301 - val_loss: 0.9530 - val_accuracy: 0.6248 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "307/307 [==============================] - 29s 95ms/step - loss: 0.9229 - accuracy: 0.6373 - val_loss: 0.9425 - val_accuracy: 0.6294 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "307/307 [==============================] - 30s 99ms/step - loss: 0.8996 - accuracy: 0.6449 - val_loss: 0.9362 - val_accuracy: 0.6355 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "307/307 [==============================] - 29s 94ms/step - loss: 0.8948 - accuracy: 0.6452 - val_loss: 0.9430 - val_accuracy: 0.6338 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "307/307 [==============================] - 29s 94ms/step - loss: 0.8791 - accuracy: 0.6521 - val_loss: 0.9323 - val_accuracy: 0.6358 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "307/307 [==============================] - 29s 93ms/step - loss: 0.8716 - accuracy: 0.6567 - val_loss: 0.9362 - val_accuracy: 0.6347 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "307/307 [==============================] - 28s 93ms/step - loss: 0.8652 - accuracy: 0.6575 - val_loss: 0.9248 - val_accuracy: 0.6402 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "307/307 [==============================] - 28s 90ms/step - loss: 0.8481 - accuracy: 0.6662 - val_loss: 0.9430 - val_accuracy: 0.6482 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "307/307 [==============================] - 28s 90ms/step - loss: 0.8433 - accuracy: 0.6692 - val_loss: 0.9389 - val_accuracy: 0.6381 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "307/307 [==============================] - 28s 91ms/step - loss: 0.8398 - accuracy: 0.6667 - val_loss: 0.9316 - val_accuracy: 0.6422 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "307/307 [==============================] - 28s 91ms/step - loss: 0.8247 - accuracy: 0.6768 - val_loss: 0.9167 - val_accuracy: 0.6428 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "307/307 [==============================] - 28s 91ms/step - loss: 0.8241 - accuracy: 0.6745 - val_loss: 0.9610 - val_accuracy: 0.6373 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "307/307 [==============================] - 29s 96ms/step - loss: 0.8118 - accuracy: 0.6779 - val_loss: 0.9292 - val_accuracy: 0.6355 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "307/307 [==============================] - 28s 92ms/step - loss: 0.8016 - accuracy: 0.6855 - val_loss: 0.9630 - val_accuracy: 0.6422 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "307/307 [==============================] - 29s 93ms/step - loss: 0.7915 - accuracy: 0.6886 - val_loss: 0.9298 - val_accuracy: 0.6482 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "307/307 [==============================] - 28s 92ms/step - loss: 0.7916 - accuracy: 0.6907 - val_loss: 0.9459 - val_accuracy: 0.6474 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "306/307 [============================>.] - ETA: 0s - loss: 0.7980 - accuracy: 0.6870\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n",
      "307/307 [==============================] - 29s 93ms/step - loss: 0.7979 - accuracy: 0.6871 - val_loss: 0.9279 - val_accuracy: 0.6500 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "307/307 [==============================] - 30s 97ms/step - loss: 0.7230 - accuracy: 0.7166 - val_loss: 0.9089 - val_accuracy: 0.6549 - lr: 4.0000e-04\n",
      "Epoch 33/50\n",
      "307/307 [==============================] - 30s 97ms/step - loss: 0.7069 - accuracy: 0.7209 - val_loss: 0.9159 - val_accuracy: 0.6584 - lr: 4.0000e-04\n",
      "Epoch 34/50\n",
      "307/307 [==============================] - 30s 98ms/step - loss: 0.6987 - accuracy: 0.7232 - val_loss: 0.9207 - val_accuracy: 0.6624 - lr: 4.0000e-04\n",
      "Epoch 35/50\n",
      "307/307 [==============================] - 32s 106ms/step - loss: 0.6882 - accuracy: 0.7310 - val_loss: 0.9141 - val_accuracy: 0.6569 - lr: 4.0000e-04\n",
      "Epoch 36/50\n",
      "307/307 [==============================] - 30s 99ms/step - loss: 0.6903 - accuracy: 0.7281 - val_loss: 0.9398 - val_accuracy: 0.6549 - lr: 4.0000e-04\n",
      "Epoch 37/50\n",
      "307/307 [==============================] - 30s 97ms/step - loss: 0.6773 - accuracy: 0.7310 - val_loss: 0.9247 - val_accuracy: 0.6540 - lr: 4.0000e-04\n",
      "Epoch 38/50\n",
      "307/307 [==============================] - 29s 95ms/step - loss: 0.6689 - accuracy: 0.7391 - val_loss: 0.9307 - val_accuracy: 0.6578 - lr: 4.0000e-04\n",
      "Epoch 39/50\n",
      "307/307 [==============================] - 29s 93ms/step - loss: 0.6675 - accuracy: 0.7395 - val_loss: 0.9263 - val_accuracy: 0.6549 - lr: 4.0000e-04\n",
      "Epoch 40/50\n",
      "307/307 [==============================] - 29s 94ms/step - loss: 0.6603 - accuracy: 0.7388 - val_loss: 0.9478 - val_accuracy: 0.6595 - lr: 4.0000e-04\n",
      "Epoch 41/50\n",
      "307/307 [==============================] - 31s 100ms/step - loss: 0.6610 - accuracy: 0.7383 - val_loss: 0.9336 - val_accuracy: 0.6642 - lr: 4.0000e-04\n",
      "Epoch 42/50\n",
      "307/307 [==============================] - 30s 98ms/step - loss: 0.6467 - accuracy: 0.7463 - val_loss: 0.9509 - val_accuracy: 0.6590 - lr: 4.0000e-04\n",
      "Epoch 43/50\n",
      "307/307 [==============================] - 29s 93ms/step - loss: 0.6442 - accuracy: 0.7444 - val_loss: 0.9446 - val_accuracy: 0.6578 - lr: 4.0000e-04\n",
      "Epoch 44/50\n",
      "307/307 [==============================] - 30s 97ms/step - loss: 0.6404 - accuracy: 0.7498 - val_loss: 0.9583 - val_accuracy: 0.6610 - lr: 4.0000e-04\n",
      "Epoch 45/50\n",
      "307/307 [==============================] - 41s 134ms/step - loss: 0.6336 - accuracy: 0.7532 - val_loss: 0.9470 - val_accuracy: 0.6581 - lr: 4.0000e-04\n",
      "Epoch 46/50\n",
      "307/307 [==============================] - 58s 190ms/step - loss: 0.6235 - accuracy: 0.7556 - val_loss: 0.9416 - val_accuracy: 0.6581 - lr: 4.0000e-04\n",
      "Epoch 47/50\n",
      "307/307 [==============================] - 65s 212ms/step - loss: 0.6319 - accuracy: 0.7535 - val_loss: 0.9619 - val_accuracy: 0.6537 - lr: 4.0000e-04\n",
      "Epoch 48/50\n",
      "306/307 [============================>.] - ETA: 0s - loss: 0.6249 - accuracy: 0.7557\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.\n",
      "307/307 [==============================] - 48s 155ms/step - loss: 0.6249 - accuracy: 0.7557 - val_loss: 0.9431 - val_accuracy: 0.6636 - lr: 4.0000e-04\n",
      "Epoch 49/50\n",
      "307/307 [==============================] - 46s 150ms/step - loss: 0.5951 - accuracy: 0.7707 - val_loss: 0.9558 - val_accuracy: 0.6590 - lr: 1.6000e-04\n",
      "Epoch 50/50\n",
      "307/307 [==============================] - 40s 129ms/step - loss: 0.5872 - accuracy: 0.7696 - val_loss: 0.9571 - val_accuracy: 0.6639 - lr: 1.6000e-04\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "rlrp = ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=1, patience=2, min_lr=0.000001)\n",
    "history2 = model2.fit(x_train_t, y_train_t, batch_size=64, epochs=50, validation_split=0.15, callbacks=[rlrp])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\n",
      "......conv1d\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv1d_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv1d_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dropout\n",
      ".........vars\n",
      "......dropout_1\n",
      ".........vars\n",
      "......flatten\n",
      ".........vars\n",
      "......max_pooling1d\n",
      ".........vars\n",
      "......max_pooling1d_1\n",
      ".........vars\n",
      "......max_pooling1d_2\n",
      ".........vars\n",
      "...metrics\n",
      "......mean\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......mean_metric_wrapper\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "...optimizer\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........10\n",
      ".........11\n",
      ".........12\n",
      ".........13\n",
      ".........14\n",
      ".........15\n",
      ".........16\n",
      ".........17\n",
      ".........18\n",
      ".........19\n",
      ".........2\n",
      ".........20\n",
      ".........3\n",
      ".........4\n",
      ".........5\n",
      ".........6\n",
      ".........7\n",
      ".........8\n",
      ".........9\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2023-01-02 20:03:05         4087\n",
      "metadata.json                                  2023-01-02 20:03:05           64\n",
      "variables.h5                                   2023-01-02 20:03:05      3089552\n"
     ]
    }
   ],
   "source": [
    "pickle.dump(model2, open('nn_model2.pkl', 'wb'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras model archive loading:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2023-01-02 20:03:04         4087\n",
      "metadata.json                                  2023-01-02 20:03:04           64\n",
      "variables.h5                                   2023-01-02 20:03:04      3089552\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
      "...layers\n",
      "......conv1d\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv1d_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv1d_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dropout\n",
      ".........vars\n",
      "......dropout_1\n",
      ".........vars\n",
      "......flatten\n",
      ".........vars\n",
      "......max_pooling1d\n",
      ".........vars\n",
      "......max_pooling1d_1\n",
      ".........vars\n",
      "......max_pooling1d_2\n",
      ".........vars\n",
      "...metrics\n",
      "......mean\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......mean_metric_wrapper\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "...optimizer\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........10\n",
      ".........11\n",
      ".........12\n",
      ".........13\n",
      ".........14\n",
      ".........15\n",
      ".........16\n",
      ".........17\n",
      ".........18\n",
      ".........19\n",
      ".........2\n",
      ".........20\n",
      ".........3\n",
      ".........4\n",
      ".........5\n",
      ".........6\n",
      ".........7\n",
      ".........8\n",
      ".........9\n",
      "...vars\n",
      "/Users/davidtrafela/PycharmProjects/SU-emotion-recognition/my_audio/David_neutral.m4a\n",
      "neutral\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "[[0. 0. 1. 0. 0. 1. 1.]]\n",
      "/Users/davidtrafela/PycharmProjects/SU-emotion-recognition/my_audio/Hi_David_sad.m4a\n",
      "sad\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "[[0. 0. 1. 0. 1. 1. 0.]]\n",
      "/Users/davidtrafela/PycharmProjects/SU-emotion-recognition/my_audio/David_angry.m4a\n",
      "angry\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "[[1. 0. 1. 0. 0. 1. 0.]]\n",
      "/Users/davidtrafela/PycharmProjects/SU-emotion-recognition/my_audio/David_sad.m4a\n",
      "sad\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "[[0. 0. 1. 0. 0. 1. 1.]]\n",
      "/Users/davidtrafela/PycharmProjects/SU-emotion-recognition/my_audio/Hi_David_happy.m4a\n",
      "happy\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "[[0. 0. 1. 0. 0. 1. 1.]]\n",
      "/Users/davidtrafela/PycharmProjects/SU-emotion-recognition/my_audio/Hi_David_neutral.m4a\n",
      "neutral\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "[[0. 0. 1. 0. 1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "clf = pickle.load(open('nn_model2.pkl', 'rb'))\n",
    "predict_for_model(clf, scaler, my_audio_df, nn=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241/241 [==============================] - 3s 13ms/step - loss: 0.9797 - accuracy: 0.6532\n",
      "[0.979719340801239, 0.6532152891159058]\n",
      "Accuracy of our model2 on test data :  65.32152891159058 %\n"
     ]
    }
   ],
   "source": [
    "evaluation2 = model2.evaluate(x_test_t, y_test_t)\n",
    "print(evaluation2)\n",
    "print(\"Accuracy of our model2 on test data : \", evaluation2[1] * 100, \"%\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs = [i for i in range(50)]\n",
    "fig, ax = plt.subplots(2, 1)\n",
    "train_acc = history2.history['accuracy']\n",
    "train_loss = history2.history['loss']\n",
    "test_acc = history2.history['val_accuracy']\n",
    "test_loss = history2.history['val_loss']\n",
    "\n",
    "fig.set_size_inches(10, 10)\n",
    "ax[0].plot(epochs, train_loss, label='Training Loss')\n",
    "ax[0].plot(epochs, test_loss, label='Validattion Loss')\n",
    "ax[0].set_title('Training & Validation Loss')\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel(\"Epochs\")\n",
    "\n",
    "ax[1].plot(epochs, train_acc, label='Training Accuracy')\n",
    "ax[1].plot(epochs, test_acc, label='Validation Accuracy')\n",
    "ax[1].set_title('Training & Validation Accuracy')\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel(\"Epochs\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
